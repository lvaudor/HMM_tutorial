[
  {
    "objectID": "case_studies.html#river-geomorphology-example",
    "href": "case_studies.html#river-geomorphology-example",
    "title": "4  Case studies",
    "section": "4.1 River geomorphology example",
    "text": "4.1 River geomorphology example\n\n4.1.1 Description of the dataset\nLet’s consider a qualitative geomorphological descriptor a river. This indicator has been built through a hierarchical clustering analysis based on several quantitative variables, namely:\n\nf.slope: log(slope)\nf.sinuosity: log(sinuosity -1)\nf.vall_bott_W: log(valley bottom width)\nf.wat_cov: logit(area under water divided by area of the active channel)\nf.act_chan_cov: log(area of the active channel divided by catchment area)\nf.act_chan_vW: log(interquartile range of active channel width)\n\nThe individuals are homogeneous reaches of average length 1.7km.\n\ndata_river &lt;- read.csv(\"data/data_river.csv\", sep=\";\")\nattach(data_river)\nx=1:nrow(data_river)\nrequire(ade4)\n\nLe chargement a nécessité le package : ade4\n\ndata=data_river[,2:ncol(data_river)]\ndatascaled=scale(data,center=TRUE)\n#boxplot(datascaled)\nmonACP=dudi.pca(data,scannf=FALSE,nf=2)\n\ntree=hclust(dist(datascaled), method=\"ward\")\n\nLa méthode \"ward\" a été renommée \"ward.D\"; notez la nouvelle méthode \"ward.D2\"\n\nlcol2=c(\"darkorchid\",\"palegreen\")\nlcol5=c(\"slateblue2\",\"maroon1\",\n       \"gold1\",\"darkolivegreen1\",\n       \"mediumaquamarine\")\n#plot(tree)grouping5=as.factor(cutree(tree,k=5))\n\ngrouping5=as.factor(cutree(tree,k=5))\ngrouping2=as.factor(cutree(tree,k=2))\n\nseries5=paste0(\"class\",grouping5)\nseries2=paste0(\"class\",grouping2)\n\n\n\n\n\n\nFigure 4.1: Description of classes. A Principal Component Analysis has been performed to help with the description.\n\n\n\n\nFigure Figure 4.1 shows the quantitative geomorphological features of the 5 classes defined through hierarchical clustering.\n\n\n\n\n\nFigure 4.2: Description of classes\n\n\n\n\nFigure Figure 4.2 displays the sequence of classes along the river.\n\n\n4.1.2 Fit of a HMM\nWe hypothesize that there are actually two hidden states (let’s call them “A” and “B”) behind the observed sequence of series). The proportions of classes in the observation series is:\n\nmyprop=round(table(series5)/length(series5),2)\nprint(myprop)\n\nseries5\nclass1 class2 class3 class4 class5 \n  0.25   0.11   0.29   0.25   0.09 \n\nmyprop=as.vector(myprop)\n\nWe initialize the emission matrix and transition matrix:\n\nM_E=matrix(c(myprop[1]+0.05,myprop[1]-0.05,\n             myprop[2]-0.05,myprop[2]+0.05,\n             myprop[3]+0.05,myprop[3]-0.05,\n             myprop[4]-0.05,myprop[4]+0.05,\n             myprop[5]+0.05,myprop[5]-0.05),nrow=2)\nrownames(M_E)=c(\"A\",\"B\")\ncolnames(M_E)=paste0(\"class\",1:5)\nM_E\n\n  class1 class2 class3 class4 class5\nA    0.3   0.06   0.34    0.2   0.14\nB    0.2   0.16   0.24    0.3   0.04\n\n\n\nM_T=matrix(c(.9,.1,.1,.9),2)\nrownames(M_T)=c(\"A\",\"B\")\ncolnames(M_T)=c(\"A\",\"B\")\nM_T\n\n    A   B\nA 0.9 0.1\nB 0.1 0.9\n\n\nThus we can initialize the HMM :\n\ninitial_HMM=initHMM(States=c(\"A\",\"B\"),\n                    Symbols=paste0(\"class\",1:5),\n                    transProbs=M_T,\n                    emissionProbs=M_E)\n\nWe can now fit the HMM model using this initial parameterization.\n\nhmm_fit= baumWelch(initial_HMM,series5,maxIterations=100)$hmm\ncolstates=c(\"cadetblue3\",\"sienna2\")\n\n\nround(hmm_fit$emissionProbs,2)\n\n      symbols\nstates class1 class2 class3 class4 class5\n     A    0.6   0.13   0.03   0.06   0.18\n     B    0.0   0.10   0.48   0.40   0.02\n\nround(hmm_fit$transProbs,2)\n\n    to\nfrom    A    B\n   A 0.93 0.07\n   B 0.04 0.96\n\n\nFigure Figure 4.3 illustrates the differences between states A and B: A is characterized by a frequent occurence of classes 3 and 4 while B is characterized by a frequent occurence of classes 1, 2 and 5.\n\nlayout(matrix(1:2,nrow=1))\npar(las=1)\nmosaicplot(hmm_fit$emissionProbs, col=lcol5)\nmosaicplot(hmm_fit$transProbs, col=colstates)\n\n\n\n\nFigure 4.3: Fitted parameters of the HMM.\n\n\n\n\nNow, we use the fitted HMM to infer the hidden states series:\n\nseriesHMM=as.factor(viterbi(hmm_fit,series5))\n\n\n\n\n\n\nFigure 4.4: 1) observation series (classification into 5 clusters based on quantitative variables) 2) hidden states series (classification into two states based on observation series consisting of 5 possible clusters).\n\n\n\n\nFigure Figure 4.4 displays the observation and estimated states series.\n\n\n4.1.3 Discussion: HMM segmentation vs clustering\nA HMM is primarily designed to describe the sequence of hidden states underlying the observation of a series. Doing so, it cuts a qualitative signal into homogeneous segments.\nIn this example, the data at hand has been used to first define 5 classes, and then a HMM model has been carried out to define 2 states based on these 5 classes. One can wonder why not define 2 classes directly.\nActually, these two methods, although they seemingly result in the same kind of output (i.e. a 2-classes segmentation of the river) do not proceed according to the same principles.\nHierarchical clustering makes no direct use in the geographical information in the data. The fact that two successive reaches fall into the same class is only due to the fact that two reaches tend to have similar geomorphological descriptions when they are close to each other.\nOn the other hand, a HMM model uses both geomorphological descriptors AND geographical information (i.e. the location of reaches in the sequence) to define which state they are most likely to result from.\nTo illustrate this we will use the quantitative data at hand to define a 2-classes description based on a hierarchical clustering. Figure Figure 4.5 shows the characteristics of these 2 classes as well as the characteristics of reaches occurring presumably in states A and B.\n\n\n\n\n\nFigure 4.5: Comparison of 1) classes (output of hierarchical clustering with 2 classes) and 2) hidden states as estimated based on a HMM model.\n\n\n\n\nFigure Figure 4.5 illustrates the closeness (in terms of descriptors) of classes either inferred (based on several quantitative variables) through hierarchical clustering and states based on a 5-classes descriptor as defined through a HMM model fit.\nOn the other hand, figure Figure 4.6 shows that there are some differences between segments either defined as the projection of the 2-classes categorization along the river (for hierarchical clustering) or defined as the most probable states path (for HMM). Taking into account the geographical succession of reaches and not only geomorphological features, the HMM path provides a less fragmented segmentation.\n\n\n\n\n\nFigure 4.6: Comparison of 1) observation series (classification into 5 clusters based on quantitative variables) 2) observation series (classification into 2 clusters based on quantitative variables) 3) hidden states series (classification into two states based on observation series consisting of 5 possible clusters)."
  },
  {
    "objectID": "case_studies.html#textual-analysis-example",
    "href": "case_studies.html#textual-analysis-example",
    "title": "4  Case studies",
    "section": "4.2 Textual analysis example",
    "text": "4.2 Textual analysis example\nLet’s consider an extract of a Sherlock Holmes novel.\n\ntext=scan(\"data/sherlock.txt\", what=\"raw\", sep=\" \")\nprint(length(text))\n\n[1] 776\n\nprint(text[1:30])\n\n [1] \"To\"           \"Sherlock\"     \"Holmes\"       \"she\"          \"is\"          \n [6] \"always\"       \"the\"          \"woman\"        \".\"            \"I\"           \n[11] \"have\"         \"seldom\"       \"heard\"        \"him\"          \"mention\"     \n[16] \"her\"          \"under\"        \"any\"          \"other\"        \"name\"        \n[21] \".\"            \"In\"           \"his\"          \"eyes\"         \"she\"         \n[26] \"eclipses\"     \"and\"          \"predominates\" \"the\"          \"whole\"       \n\n\nWe are interested in distinguishing the parts where the focus is mainly on Sherlock Holmes, to parts where the attention is mainly focused on the narrator, Dr Watson. We want to do this on all Sherlock Holmes novels so that this distinction should be done automatically (though here we will only use a short extract as an example).\nWe categorize words into 3 categories:\n\na neutral category (outcome “blah”)\na Sherlock Holmes-centered category (outome “he”)\na John Watson-centered category (outcome “I”)\n\n\n\n           text observations\n1            To         blah\n2      Sherlock           he\n3        Holmes           he\n4           she         blah\n5            is         blah\n6        always         blah\n7           the         blah\n8         woman         blah\n9             .         blah\n10            I            I\n11         have         blah\n12       seldom         blah\n13        heard         blah\n14          him           he\n15      mention         blah\n16          her         blah\n17        under         blah\n18          any         blah\n19        other         blah\n20         name         blah\n21            .         blah\n22           In         blah\n23          his           he\n24         eyes         blah\n25          she         blah\n26     eclipses         blah\n27          and         blah\n28 predominates         blah\n29          the         blah\n30        whole         blah\n\n\nFigure Figure 4.7 shows how the sequence of outcomes looks like for our example.\n\n\n\n\n\nFigure 4.7: The observation series: each point corresponds to a word in a text. Blue points correspond to an occurrence of an ‘I’ outcome, while red points correspond to an occurrence of a ‘he’ outcome.\n\n\n\n\n\nmyprop=table(observations)\nmyprop=myprop/sum(myprop)\nprint(myprop)\n\nobservations\n      blah         he          I \n0.90592784 0.06185567 0.03221649 \n\nmyprop=as.vector(myprop)\n\nWe initialize the emission matrix stating that occurrence of the “he” outcome should be more frequent in the Holmes-centered parts while the “I” will be more frequent in the Watson-centered parts.\n\nM_E=matrix(c(myprop[1], myprop[1],\n             myprop[2]+0.01, myprop[2]-0.01,\n             myprop[3]-0.01, myprop[3]+0.01),\n             nrow=2)\nrownames(M_E)=c(\"Holmes\",\"Watson\")\ncolnames(M_E)=c(\"blah\",\"he\",\"I\")\n\nWe initialize the transition matrix stating that transition from a state to another happens about 1 time out of 100 words:\n\nM_T=matrix(c(0.99,0.01,0.01,0.99),2)\nrownames(M_T)=c(\"Holmes\",\"Watson\")\ncolnames(M_T)=c(\"Holmes\",\"Watson\")\n\nThe initial HMM is:\n\ninitial_HMM=initHMM(States=c(\"Holmes\",\"Watson\"),\n                    Symbols=c(\"blah\",\"he\",\"I\"),\n                    transProbs=M_T,\n                    emissionProbs=M_E)\n\nWe can now fit the HMM model using this initial parameterization.\n\nhmm_fit= baumWelch(initial_HMM,observations,maxIterations=100)$hmm\n\ncol_outcomes=c(\"grey\",\"red\",\"blue\")\ncol_states=c(\"pink2\",\"skyblue3\")\n\nprint(round(hmm_fit$emissionProbs,2))\n\n        symbols\nstates   blah   he    I\n  Holmes 0.91 0.07 0.02\n  Watson 0.90 0.00 0.10\n\nprint(round(hmm_fit$transProbs,2))\n\n        to\nfrom     Holmes Watson\n  Holmes   1.00   0.00\n  Watson   0.01   0.99\n\n\nThe characteristics of states “Holmes” and “Watson” are displayed in figure Figure 4.8. Watson (outcome “I”), as a narrator, never completely disappears from the text even in Holmes-centered parts, while Watson-parts are characterized by a null occurrence of outcome “he”.\n\n\n\n\n\nFigure 4.8: Fitted parameters of the HMM.\n\n\n\n\nNow, we use the fitted HMM to infer the hidden states series:\n\nstates_speech=as.factor(viterbi(hmm_fit,observations))\n\n\n\n\n\n\nFigure 4.9: Observation series and state series.\n\n\n\n\nFigure Figure 4.9 displays the observation and estimated states series. There is only one part of the text which appears to be Watson-centered (according to how Watson-centerization is defined, i.e. total disappearance of “he” outcomes.)\n\nprint(text[states_speech==\"Watson\"])\n\n [1] \"I\"          \"merely\"     \"shared\"     \"with\"       \"all\"       \n [6] \"the\"        \"readers\"    \"of\"         \"the\"        \"daily\"     \n[11] \"press\"      \",\"          \"I\"          \"knew\"       \"little\"    \n[16] \"of\"         \"my\"         \"former\"     \"\"           \"friend\"    \n[21] \"and\"        \"companion\"  \".\"          \"One\"        \"night\"     \n[26] \"-\"          \"it\"         \"was\"        \"on\"         \"the\"       \n[31] \"twentieth\"  \"of\"         \"March\"      \",\"          \"1888\"      \n[36] \"-\"          \"I\"          \"was\"        \"returning\"  \"from\"      \n[41] \"a\"          \"journey\"    \"to\"         \"a\"          \"patient\"   \n[46] \"(\"          \"for\"        \"I\"          \"had\"        \"now\"       \n[51] \"returned\"   \"to\"         \"civil\"      \"practice\"   \")\"         \n[56] \",\"          \"when\"       \"my\"         \"way\"        \"led\"       \n[61] \"me\"         \"through\"    \"Baker\"      \"Street\"     \".\"         \n[66] \"As\"         \"I\"          \"passed\"     \"the\"        \"well\"      \n[71] \"-\"          \"remembered\" \"door\"       \",\"          \"which\"     \n[76] \"must\"       \"always\"     \"be\"         \"associated\" \"in\"        \n[81] \"my\"         \"mind\"       \"with\"       \"my\"         \"wooing\"    \n[86] \",\"          \"and\"        \"with\"       \"the\"        \"dark\"      \n[91] \"incidents\"  \"of\"         \"the\"        \"Study\"      \"in\"        \n[96] \"Scarlet\"    \",\"          \"I\""
  },
  {
    "objectID": "intro.html#a-generic-example-of-hmm-the-bags-and-balls-colors-series",
    "href": "intro.html#a-generic-example-of-hmm-the-bags-and-balls-colors-series",
    "title": "1  Introduction",
    "section": "1.2 A generic example of HMM: the bags and balls’ colors series",
    "text": "1.2 A generic example of HMM: the bags and balls’ colors series\n\n\n\nAn example of Hidden Markov Model with two states (bag A and bag B) and two outcomes (red ball -R- or white ball -W).\n\n\nLet’s consider the following example of HMM (which we will refer to as the bags and balls’s colors example throughout this tutorial).\nAn operator has two bags, A et B, filled with either red or white balls in different proportions. He carries out the following experiment:\n\nHe chooses one bag out of the two\nHe draws a ball out the bag he has just chosen, writes down its color on a sheet of paper, and then puts it back in the bag\nHe chooses to keep that same bag or to take the other one\nHe draws a ball out of the bag, writes down its color on the sheet of paper, and then puts it back in the bag\nHe chooses to keep that same bag or to take the other one,\netc.\n\nHe thus repeats the same process “drawing of ball, choosing of bag” \\(n\\) times. Then we are given the sheet of paper with the succession of colors written on it… We thus know the sequence of colors (the observations) but we do not know the sequence of bags (the hidden states). A possible series of hidden states and observations is shown in figure Figure 1.1.\nWe suppose that at any iteration \\(k\\), the operator chooses to keep the same bag or to change it only based on a constant rule which depends on the bag he had during iteration \\(k-1\\).\nThat is to say that the hidden state variable has the Markov property: its probability distribution at iteration \\(k\\) depends only on its value at iteration \\((k_1)\\), but is independent of all previous iterations (hence \\(pr(S_k/S_{k-1})\\) is independent of \\(S_1,S_2,...S_{k-2}\\)). It also means that \\(pr(S_k/S_{k-1})\\) is independent of \\(k\\).\n\n\n\nFigure 1.1: An example of hidden states and observations series"
  },
  {
    "objectID": "intro.html#notations-associated-with-a-hmm",
    "href": "intro.html#notations-associated-with-a-hmm",
    "title": "1  Introduction",
    "section": "1.3 Notations associated with a HMM",
    "text": "1.3 Notations associated with a HMM\nFinally we know the series \\(X=(X_1,X_2,X_3,...,X_n)\\) which corresponds, in the bags and balls example, to the balls’ colors (i.e. the observations series). The two possible outcomes are “red” (R) or “white” (W). On the other hand, we do not know the series \\(S=(S_1,S_2,S_3,...S_n)\\) which corresponds to the names of the bags out of which the balls have been drawn (i.e. the hidden states series)\nHere, the possible hidden states are either \\(S_k=A\\) ou \\(S_k=B\\) and the possible outcomes are either \\(X_k=R\\) ou \\(X_k=W\\).\n\n1.3.1 Transition probabilities\nLet’s consider the following notations:\nFor all iterations (i.e. for all values of \\(k\\)), the probabilities of a transition from a hidden state to another are:\n\n\\(p_{AB}=pr(S_k=B/S_{k-1}=A)\\) is the probability that the operator changes from bag A to bag B\n\\(p_{AA}=pr(S_k=A/S_{k-1}=A)\\) is the probability that the operator keeps bag A\n\\(p_{BA}=pr(S_k=A/S_{k-1}=B)\\) is the probability that the operator changes from bag B to bag A\n\\(p_{BB}=pr(S_k=B/S_{k-1}=B)\\) is the probability that the operator keeps bag B\n\nBased on these notations we can define the transition matrix (transition from a state to another) as:\n\n\n\n\nA\nB\n\n\n\n\nA\n\\(p_{AA}\\)\n\\(p_{AB}\\)\n\n\nB\n\\(p_{BA}\\)\n\\(p_{BB}\\)\n\n\n\nGeneralization\nLet \\(S_k\\) be a discrete hidden random variable with sample space (i.e. set of all \\(n_S\\) possible outcomes) \\(\\Omega_S\\).\nThe probability of a state \\(j\\) at time \\(k\\) given the state was \\(i\\) at time \\(k-1\\) is given by matrix:\n\\[\nT=\\{t_{ij}\\}_{(i,j)\\in \\Omega_S^2}=pr(S_k=j|S_{k-1}=i)\n\\]\n\n\n1.3.2 Emission probabilities\nFor all iterations (i.e. for all values of \\(k\\)), the probabilities of the outcomes conditional on hidden state are:\n\n\\(p_{AW}=pr(X_k=W/S_k=A)\\) is the probability of drawing a white ball from bag A\n\\(p_{AR}=pr(X_k=R/S_k=A)\\) is the probability of drawing a red ball from bag A\n\\(p_{BW}=pr(X_k=W/S_k=B)\\) is the probability of drawing a white ball from bag B\n\\(p_{BR}=pr(X_k=R/S_k=B)\\) is the probability of drawing a red ball from bag B\n\nBased on these notations we can define the emission matrix (emission of an outcome from a state) as:\n\n\n\n\nW\nR\n\n\n\n\nA\n\\(p_{AW}\\)\n\\(p_{AR}\\)\n\n\nB\n\\(p_{BW}\\)\n\\(p_{BR}\\)\n\n\n\nGeneralization\nLet \\(X_k\\) be a discrete observed random variable with sample space (i.e. set of all \\(n_X\\) possible outcomes) \\(\\Omega_X\\).\nThe probability of a certain outcome \\(l\\) at time \\(k\\) given the state at that same time is \\(i\\) is given by matrix:\n\\[\nE=\\{e_{il}\\}_{(i,l)\\in \\Omega_S\\times\\Omega_X}=pr(X_k=l|S_k=i)\n\\]\n\n\n1.3.3 Initial state distribution\nThe initial state distribution (i.e. probabilities of being in state i when \\(k=1\\)) is given by\n\\[\n\\pi_i=pr(S_1 = i).\n\\]\nWe use the notation \\(\\pi=(\\pi_1,\\pi_2,...\\pi_{n_X})\\)\n\n\n1.3.4 Parameters of a HMM\nFinally, a hidden Markov model can be fully described by the set of parameters \\(\\theta = (T,E,\\pi)\\) where\n\n\\(T\\) corresponds to the transition probabilities,\n\\(E\\) corresponds to the emission probabilities,\n\\(\\pi\\) to the initial state probabilities."
  },
  {
    "objectID": "intro.html#questions-associated-to-a-hmm",
    "href": "intro.html#questions-associated-to-a-hmm",
    "title": "1  Introduction",
    "section": "1.4 Questions associated to a HMM",
    "text": "1.4 Questions associated to a HMM\nA HMM user typically searches answers to the following questions:\n\nWhat are the characteristics of states, which optimally describe the series of observations?\nWhat is the most probable series of hidden states considering the series of observations?\n\nAnswering the first question consists in fitting the model, i.e. estimating its parameters (emission and transition probabilities). This will be the focus of section Chapter 2.\nAnswering the second one consists in calculating the most likely series of hidden states (based on both observation series and fitted model). This will be the focus of section Chapter 3."
  },
  {
    "objectID": "intro.html#a-practical-example-the-wood-occurrence-series",
    "href": "intro.html#a-practical-example-the-wood-occurrence-series",
    "title": "1  Introduction",
    "section": "1.5 A practical example: the wood occurrence series",
    "text": "1.5 A practical example: the wood occurrence series\nLet’s consider a spatial series consisting of occurrence of wood rafts along a river. Each data sample corresponds to a 500 meters-long section of the river.\n\nwood&lt;- read.csv(\"data/wood.csv\", sep=\";\", header=T)\nattach(wood)\nprint(wood[1:10,])\n\n    PK obsWood\n1  0.0 nothing\n2  0.5 nothing\n3  1.0 nothing\n4  1.5    wood\n5  2.0    wood\n6  2.5 nothing\n7  3.0 nothing\n8  3.5    wood\n9  4.0 nothing\n10 4.5 nothing\n\n\nIn the script tools_cat_series you will find some R functions helping with the graphical representation of categorical series.\n\nsource(\"scripts/tools_cat_series.R\")\ncol_outcomes=c(\"slateblue4\",\"burlywood3\")\nplot_cat_series(obsWood,x=PK,col=col_outcomes)\n\n\n\n\nAn example of observation series: occurrence of wood 500 meter-long sections of the river.\n\n\n\n\nWe want to characterize the fact that the occurrence of wood (“wood” outcome vs “nothing” outcome) is more or less frequent according to an unobserved characteristic of the river. Hence we consider the river is either in a state we define as:\n\n“State 1: High frequency of wood occurrence” (H) or\n“State 2: Low frequency of wood occurrence” (L).\n\nOver the whole series of observations :\n\nprint(table(obsWood))\n\nobsWood\nnothing    wood \n    100      88 \n\npw=table(obsWood)[2]/length(obsWood)\nprint(pw)\n\n     wood \n0.4680851 \n\n\nWe have 46.81% chances of observing the “wood” outcome on a single point.\n\n\n\n\n\nFigure 1.2: Frequency of wood occurrence along the river (20 equal-sized sections). The horizontal dotted lines correspond to the quantiles of order 2.5% and 97.5% of the observed proportion under the hypothesis of a contant value \\(\\pi_w\\) along the river.\n\n\n\n\nCutting the river into (for instance) 20 equal-sized sections, we observe that the frequency of “wood” outcome seems to vary along the river (cf fig. Figure 1.2). If obsWood followed a constant binomial distribution (with a proportion of “wood” outcome \\(\\pi_w=\\) 46.81%) all along the river, then we would have a 95% probability of observing proportions of “wood” outcome between 37% and 57%."
  },
  {
    "objectID": "fitting.html#the-baum-welch-algorithm",
    "href": "fitting.html#the-baum-welch-algorithm",
    "title": "2  Fitting a HMM",
    "section": "2.1 The Baum-Welch algorithm",
    "text": "2.1 The Baum-Welch algorithm\nThe Baum–Welch algorithm is used to find the unknown parameters \\(\\theta = (T,E,\\pi)\\) of a HMM, which maximizes the probability of the observation series.\n\\[\n\\widehat\\theta = \\max_{\\theta} pr(X/\\theta)\n\\]\nLet’s consider random initial values (or values set according to prior knowledge) for \\(\\theta\\): \\(\\theta_0 = (T_0, E_0, \\pi)\\).\n\n2.1.1 Forward and backward procedures\n\n\n\nFigure 2.1: Forward and backward probabilities\n\n\nForward procedure\n\\[\n\\alpha_i(k)=pr(X_1=x_1,...,X_k=x_k,S_k=i/\\theta)\n\\]\nis the probability of getting the “beginning” of the observation series \\(x_1,x_2,...,x_k\\) and having \\(i\\) as ending state (at time \\(k\\)) given parameters \\(\\theta\\).\n\\(\\alpha_i(k)\\) is calculated recursively, with: \\[\n\\alpha_i(1)=\\pi_i e_{ix_1}\\\\\n\\alpha_j(k+1)=e_{jx_{k+1}} \\sum_{i=1}^{n_S}\\alpha_i(k) t_{ij}\n\\]\nIndeed, the probability of the series \\(x_1,x_2,...,x_k+1\\) and state \\(j\\) at time \\(k+1\\) is the probability that (considering all possible states \\(i\\) at time \\(k\\)):\n\nthe first \\(k\\) observations were \\(x_1,x_2,...,x_k\\) and the state was \\(i\\) at time \\(k\\)\n\nthere has been a transition from state \\(i\\) to state \\(j\\) at time \\(k\\)\nthe outcome at time \\(k+1\\) is \\(x_k+1\\)\n\nBackward procedure\n\\[\n\\beta_i(k)=pr(X_{k+1}=x_{k+1},...,X_n=x_n/S_k=i,\\theta)\n\\] is the probability of getting the “ending” of the observation series \\(x_{k+1},...,x_{n}\\) and \\(i\\) as starting state (at time \\(k\\)) given parameters \\(\\theta\\).\n\\(\\beta_i(k)\\) is calculated recursively, with: \\[\n\\beta_i(T)=1\\\\\n\\beta_i(k)=\\sum_{j=1}^{n_S} \\beta_j(k+1)t_{ij}e_j(x_{k+1})\n\\]\nIndeed, the probability of the series \\(x_{k},x_{k+1},...,x_n\\) and state \\(i\\) at time \\(k\\) is the probability that (considering all possible states \\(j\\) at time \\(k+1\\)):\n\nthe last \\(n-k\\) observations were \\(x_{k+1},...,x_n\\) and the state was \\(j\\) at time \\(k+1\\)\n\nthere has been a transition from state \\(i\\) to state \\(j\\) at time \\(k\\)\nthe outcome at time \\(k+1\\) is \\(x_k+1\\)\n\nThe forward and backward procedures are illustrated in figure Figure 2.1.\n\n\n2.1.2 Probability of states\nWe can now calculate the probability of being in state \\(i\\) at time \\(k\\) given the observed sequence \\(X\\) and parameters \\(\\theta\\), \\(\\gamma_i(k)=pr(S_k=i/X,\\theta)\\).\nIndeed, according to Bayes’s formula,\n\\[\npr(S_k=i/X,\\theta)=\\frac{pr(X,S_k=i/\\theta)pr(\\theta)}{pr(X,\\theta)}\n\\]\n\\[\n=\\frac{pr(X,S_k=i/\\theta)pr(\\theta)}{pr(X/\\theta)pr(\\theta)}\n\\] \\[\n=\\frac{pr(X,S_k=i/\\theta)}{pr(X/\\theta)}\n\\tag{2.1}\\]\nand we have\n\\[\npr(X,S_k=i/\\theta) = pr(X_1,X_2,....X_k,S_k=i/\\theta)* pr(X_{k+1},X_{k+2},....X_n,S_{k+1}=j/\\theta)\n\\]\n\\[\n= \\alpha_i(k)\\beta_i(k)\n\\tag{2.2}\\]\nSumming this probability for all possible states at time \\(k\\) we get:\n\\[\npr(X/\\theta) = \\sum_{m=1}^{n_S}pr(X,S_k=m)/\\theta)\n\\]\n\\[\n= \\sum_{m=1}^{n_S}\\alpha_m(k)\\beta_m(k)    \n\\tag{2.3}\\]\nHence with results Equation 2.2 and Equation 2.3 we can simplify equation Equation 2.1 into:\n\\[\n\\gamma_i(k) = pr(S_k=i/X,\\theta)\n\\] \\[\n= \\frac{\\alpha_i(k)\\beta_i(k)}\n                {\\sum_{m=1}^{n_S}\\alpha_m(k)\\beta_m(k)}\n\\] We can now calculate the probability of being in state \\(i\\) at time \\(k\\) given the observed sequence \\(X\\) and parameters \\(\\theta\\), \\(\\gamma_i(k)=pr(S_k=i/X,\\theta)\\).\nThe probability of being in state \\(i\\) and \\(j\\) at times \\(k\\) and \\(k+1\\) respectively given the observed sequence \\(X\\) and parameters \\(\\theta\\), \\(\\xi_{ij}(k)=pr(S_k=i,S_{k+1}=j/X,\\theta)\\).\nIndeed, according to Bayes’s formula\n\\[\npr(S_k=i,S_{k+1}=j/X,\\theta)=\\frac{pr(X,S_k(i),S_{k+1}=j/\\theta)pr(\\theta)}{pr(X,\\theta)}\n\\]\n\\[\n=\\frac{pr(X,S_k(i),S_{k+1}=j/\\theta)pr(\\theta)}\n                                  {pr(X/\\theta)pr(\\theta)}\n\\] \\[\n=\\frac{pr(X,S_k(i),S_{k+1}=j/\\theta)}                             {pr(X/\\theta)}\n\\tag{2.4}\\]\nand we can reformulate the probability of observations and successive states at times \\(k\\) and \\(k+1\\) the following way:\n\\[\npr(X,S_k=i,S_{k+1}=j)/\\theta) = pr(X_1,X_2,...,X_k, S_k=i/\\theta)* pr(X_{k+1}, S_k=i, S_{k+1}=j/\\theta)*pr(X_{k+2},X_{k+3},...,X_n, S_k+1=j/\\theta)\n\\]\n\\[\n= \\alpha_i(k)t_{ij}e_{jx_{k+1}}\\beta_j(k+1)\n\\tag{2.5}\\]\nHence with results Equation 2.5 and Equation 2.3 we can simplify equation Equation 2.4 into:\n\\[\n\\xi_{ij}(k) = pr(S_k=i,S_{k+1}=j/X,\\theta)\n\\]\n\\[\n= \\frac{\\alpha_i(k)t_{ij}e_{jx_{k+1}}\\beta_j(k+1)}\n                {\\sum_{m=1}^{n_S}\\alpha_m(k)\\beta_m(k)}       \n\\]\n\n\n2.1.3 Estimates of parameters\nThe expected frequency spent in state i at time 1 is: \\[\n\\widehat{\\pi_i} = \\gamma_i(1)\n\\]\nThe expected transition probability from state \\(i\\) to state \\(j\\) is equal to the expected frequency of states \\(i\\) and \\(j\\) at two successive times, divided by the expected frequency of state \\(i\\).\n\\[\n\\widehat{t_{ij}}=\\frac{\\sum^{n-1}_{k=1}\\xi_{ij}(k)}{\\sum^{n-1}_{k=1}\\gamma_i(k)}\n\\]\nThe expected emission frequency of outcome \\(x\\) from state \\(i\\) is equal to the frequency of state i and outcome \\(x\\) occurring at the same time over the expected frequency of state \\(i\\).\n\\[\n\\widehat{e_{ix}}=\\frac{\\sum^n_{k=1} 1_{X_k=x} \\gamma_i(k)}{\\sum^n_{k=1} \\gamma_i(k)}\n\\]\nAs a consequence, based on \\(X\\) and \\(\\theta_0\\) (corresponding to the initial transition and emission probabilities), we can calculate and expected value \\(\\hat\\theta\\) (corresponding to transition and emission probabilities that are consistent both with observations and hypothesized values of \\(\\theta\\)).\n\nWe set \\(\\theta=\\theta_0\\) and based on these parameters calculate \\(\\theta_1=\\hat\\theta\\)\nWe set \\(\\theta=\\theta_1\\) and based on these parameters calculate \\(\\theta_2=\\hat\\theta\\)\netc.\n\nThis process is repeated until a desired level of convergence, i.e. until\\(|\\hat\\theta-\\theta|\\) is below a certain threshold. Indeed, the lower \\(|\\hat\\theta-\\theta|\\), the closer \\(\\hat\\theta\\) is to a local optimum for \\(\\theta\\). Note that the algorithm does not guarantee a global maximum."
  },
  {
    "objectID": "fitting.html#fitting-a-hmm-in-practice",
    "href": "fitting.html#fitting-a-hmm-in-practice",
    "title": "2  Fitting a HMM",
    "section": "2.2 Fitting a HMM in practice",
    "text": "2.2 Fitting a HMM in practice\nLet’s go back to the wood occurrence series. We will use the Baum-Welch algorithm to fit a HMM with two hidden states, H and L, defined as “High occurrence frequency state” and “Low occurrence frequency state”. To do this, we will use the function “baumWelch” from the “HMM” R package.\n\nlibrary(HMM)\n\nAs we have seen, the Baum-Welch algorithm iteratively adjusts the parameter values \\(\\theta\\) to maximize likelihood. Thus we have to initialize the algorithm with a prior estimate of \\(\\theta\\).\nWe could, for instance, consider that the two states we are interested in are not very contrasted (let’s talk of uncontrasted states -UCS- from now on), and that transition probabilities are quite low (low transition probabilities: LTP). Here, we will hence consider that the H state is characterized by +5% chances of getting wood, and the L state is characterized by -5% chances of getting wood (compared to the overal frequency of 46.81%).\nThus we can write the “UCS” emission matrix this way:\n\nM_UCS=matrix(c(1-(pw-0.05),1-(pw+0.05),pw-0.05,(pw+0.05)),2)\nrownames(M_UCS)=c(\"L\",\"H\")\ncolnames(M_UCS)=c(\"nothing\",\"wood\")\nprint(round(M_UCS,2))\n\n  nothing wood\nL    0.58 0.42\nH    0.48 0.52\n\n\nWe will also consider that at each time, there is only 10% chances to change from state L to state H, or from state H to state L.\nThus we can write the “LTP” transition matrix this way:\n\nM_LTP=matrix(c(.9,.1,.1,.9),2)\nrownames(M_LTP)=c(\"L\",\"H\")\ncolnames(M_LTP)=c(\"L\",\"H\")\nprint(M_LTP)\n\n    L   H\nL 0.9 0.1\nH 0.1 0.9\n\n\nThe initialization of the HMM is done through:\n\ninitial_HMM=initHMM(States=c(\"L\",\"H\"),\n                    Symbols=c(\"nothing\",\"wood\"),\n                    transProbs=M_LTP,\n                    emissionProbs=M_UCS)\nprint(initial_HMM)\n\n$States\n[1] \"L\" \"H\"\n\n$Symbols\n[1] \"nothing\" \"wood\"   \n\n$startProbs\n  L   H \n0.5 0.5 \n\n$transProbs\n    to\nfrom   L   H\n   L 0.9 0.1\n   H 0.1 0.9\n\n$emissionProbs\n      symbols\nstates   nothing      wood\n     L 0.5819149 0.4180851\n     H 0.4819149 0.5180851\n\n\ni.e. the states (L or H), and outcomes (“nothing” or “wood”) are defined, and initial values are set to the transition and emission matrices.\nThe various elements of the object initial_HMM can be accessed (as is the usual way with R lists) through:\n\nnames(initial_HMM)\n\n[1] \"States\"        \"Symbols\"       \"startProbs\"    \"transProbs\"   \n[5] \"emissionProbs\"\n\nprint(initial_HMM$emissionProbs)\n\n      symbols\nstates   nothing      wood\n     L 0.5819149 0.4180851\n     H 0.4819149 0.5180851\n\n\nThen the optimization can be carried out through:\n\nhmm_fit_1= baumWelch(initial_HMM,obsWood,maxIterations=100)\nprint(hmm_fit_1)\n\n$hmm\n$hmm$States\n[1] \"L\" \"H\"\n\n$hmm$Symbols\n[1] \"nothing\" \"wood\"   \n\n$hmm$startProbs\n  L   H \n0.5 0.5 \n\n$hmm$transProbs\n    to\nfrom         L          H\n   L 0.9151896 0.08481042\n   H 0.1010682 0.89893176\n\n$hmm$emissionProbs\n      symbols\nstates    nothing      wood\n     L 0.89814860 0.1018514\n     H 0.08898919 0.9110108\n\n\n$difference\n [1] 2.066096e-01 3.033430e-01 1.627027e-01 6.527756e-02 2.941533e-02\n [6] 1.453277e-02 7.758297e-03 4.420073e-03 2.671842e-03 1.698925e-03\n[11] 1.121976e-03 7.599168e-04 5.228788e-04 3.632509e-04 2.538315e-04\n[16] 1.780077e-04 1.251125e-04 8.805983e-05 6.203723e-05 4.373093e-05\n[21] 3.083901e-05 2.175362e-05 1.534776e-05 1.082969e-05 7.642364e-06\n[26] 5.393471e-06 3.806534e-06 2.686619e-06 1.896241e-06 1.338409e-06\n[31] 9.446910e-07 6.667992e-07 4.706559e-07 3.322112e-07 2.344913e-07\n[36] 1.655162e-07 1.168302e-07 8.246518e-08 5.820851e-08 4.108684e-08\n[41] 2.900142e-08 2.047085e-08 1.444951e-08 1.019930e-08 7.199249e-09\n[46] 5.081653e-09 3.586915e-09 2.531860e-09 1.787129e-09 1.261470e-09\n[51] 8.904208e-10"
  },
  {
    "objectID": "fitting.html#effect-of-parameter-initialization",
    "href": "fitting.html#effect-of-parameter-initialization",
    "title": "2  Fitting a HMM",
    "section": "2.3 Effect of parameter initialization",
    "text": "2.3 Effect of parameter initialization\nAccording to how we initialize the HMM, we could get different results… Indeed, the Baum-Welch algorithm converges to a local solution which might not be the global optimum (i.e., according to the initial values of the matrices, we could get different results).\nHence, let us consider different initial parameterizations. In our first example we had considered states that were quite uncontrasted (there was only +10% chances of observing some wood in state H compared to state L). Alternatively, we could consider quite contrasted states (e.g. with +40% chances of observing some wood in state H compared to state L):\n\nM_CS=matrix(c(1-(pw-0.40),1-(pw+0.40),pw-0.40,(pw+0.40)),2)\nrownames(M_CS)=c(\"L\",\"H\")\ncolnames(M_CS)=c(\"nothing\",\"wood\")\nprint(round(M_CS,2))\n\n  nothing wood\nL    0.93 0.07\nH    0.13 0.87\n\n\nIn our first example we also considered that there were few chances to change from one state from another, i.e. we used low transition probabilities. Alternatively, we could consider higher transition probabilities (HTP):\n\nM_HTP=matrix(c(.5,.5,.5,.5),2)\nrownames(M_HTP)=c(\"L\",\"H\")\ncolnames(M_HTP)=c(\"L\",\"H\")\nprint(M_HTP)\n\n    L   H\nL 0.5 0.5\nH 0.5 0.5\n\n\nLet’s initialize our HMM in various ways with matrices M_LTP vs M_HTP for transition probabilities and M_CS vs M_UCS for emission probabilities (i.e. 4 possibilities):\n\n# FIRST CASE: starting from M_LTP and M_UCS\nhmm_init_LTP_UCS = initHMM(c(\"L\",\"H\"),c(\"nothing\",\"wood\"),\n                   transProbs=M_LTP,\n                   emissionProbs=M_UCS)\n# SECOND CASE: starting from M_LTP and M_CS\nhmm_init_LTP_CS = initHMM(c(\"L\",\"H\"),c(\"nothing\",\"wood\"),\n                   transProbs=M_LTP,\n                   emissionProbs=M_CS)\n# THIRD CASE: starting from M_HTP and M_UCS\nhmm_init_HTP_UCS = initHMM(c(\"L\",\"H\"),c(\"nothing\",\"wood\"),\n                   transProbs=M_HTP,\n                   emissionProbs=M_UCS)\n# FOURTH CASE: starting from M_HTP and M_CS\nhmm_init_HTP_CS = initHMM(c(\"L\",\"H\"),c(\"nothing\",\"wood\"),\n                   transProbs=M_HTP,\n                   emissionProbs=M_CS)\n\n\nlist_hmm_init=list(hmm_init_LTP_UCS,\n                   hmm_init_LTP_CS,\n                   hmm_init_HTP_UCS,\n                   hmm_init_HTP_CS)\n\nNow the HMM are fitted (with various initial parameterizations):\n\nhmm_fit_LTP_UCS= baumWelch(hmm_init_LTP_UCS,obsWood)$hmm\nhmm_fit_LTP_CS= baumWelch(hmm_init_LTP_CS,obsWood)$hmm\nhmm_fit_HTP_UCS= baumWelch(hmm_init_HTP_UCS,obsWood)$hmm\nhmm_fit_HTP_CS= baumWelch(hmm_init_HTP_CS,obsWood)$hmm\nlist_hmm_fit=list(hmm_fit_LTP_UCS,\n                  hmm_fit_LTP_CS,\n                  hmm_fit_HTP_UCS,\n                  hmm_fit_HTP_CS)\n\nHere are the transition matrices we get in the four cases:\n\nprint(round(hmm_fit_LTP_UCS$transProbs,2))\n\n    to\nfrom    L    H\n   L 0.92 0.08\n   H 0.10 0.90\n\nprint(round(hmm_fit_LTP_CS$transProbs,2))\n\n    to\nfrom    L    H\n   L 0.92 0.08\n   H 0.10 0.90\n\nprint(round(hmm_fit_HTP_UCS$transProbs,2))\n\n    to\nfrom    L    H\n   L 0.92 0.08\n   H 0.10 0.90\n\nprint(round(hmm_fit_HTP_CS$transProbs,2))\n\n    to\nfrom    L    H\n   L 0.92 0.08\n   H 0.10 0.90\n\n\nHere are the emission matrices we get in the four cases:\n\nprint(round(hmm_fit_LTP_UCS$emissionProbs,2))\n\n      symbols\nstates nothing wood\n     L    0.90 0.10\n     H    0.09 0.91\n\nprint(round(hmm_fit_LTP_CS$emissionProbs,2))\n\n      symbols\nstates nothing wood\n     L    0.90 0.10\n     H    0.09 0.91\n\nprint(round(hmm_fit_HTP_UCS$emissionProbs,2))\n\n      symbols\nstates nothing wood\n     L    0.90 0.10\n     H    0.09 0.91\n\nprint(round(hmm_fit_HTP_CS$emissionProbs,2))\n\n      symbols\nstates nothing wood\n     L    0.90 0.10\n     H    0.09 0.91\n\n\nIn this particular case, we have obtained the same fits with all initial parameterizations (which is actually reassuring as it implies that the optimum we get is a “not-so-local” optimum).\nBut if we had provided the Baum-Welch algorithm with a very different initial transition matrix (“VHTP” for “Very High Transition Probabilities”), e.g.\n\nM_VHTP=matrix(c(.1,.9,.9,.1),2)\nrownames(M_VHTP)=c(\"L\",\"H\")\ncolnames(M_VHTP)=c(\"L\",\"H\")\nprint(M_VHTP)\n\n    L   H\nL 0.1 0.9\nH 0.9 0.1\n\nhmm_init_VHTP_UCS = initHMM(c(\"L\",\"H\"),c(\"nothing\",\"wood\"),\n                   transProbs=M_VHTP,\n                   emissionProbs=M_UCS)\nhmm_fit_VHTP_UCS= baumWelch(hmm_init_VHTP_UCS,obsWood)$hmm\n\nthen we would have got the following fit:\n\nhmm_fit_VHTP_UCS$transProbs\n\n    to\nfrom         L         H\n   L 0.1005015 0.8994985\n   H 0.8992563 0.1007437\n\nhmm_fit_VHTP_UCS$emissionProbs\n\n      symbols\nstates   nothing      wood\n     L 0.5319157 0.4680843\n     H 0.5319141 0.4680859\n\n\nThis is actually a very peculiar way to initialize the transition matrix as it assumes that there are more chances of changing states at each time than of staying in the same state. With this parameterization, chances are the series of states is basically a succession of “L”-“H”-“L”-“H”-“L”-“H”… so that the two states are naturally quite uncontrasted in terms of outcome probabilities."
  },
  {
    "objectID": "hidden_states.html#the-viterbi-algorithm",
    "href": "hidden_states.html#the-viterbi-algorithm",
    "title": "3  Predicting hidden states",
    "section": "3.1 The Viterbi algorithm",
    "text": "3.1 The Viterbi algorithm\nThe Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states – called the Viterbi path – that results in a sequence of observed events, especially in the context of Markov information sources and hidden Markov models.\nThe most likely series of states \\(s_1,\\dots,s_n\\) underlying the observations is found recursively.\n\n\n\nFigure 3.1: Viterbi algorithm\n\n\nLet us define \\(\\delta_i(k)\\) as the highest probability of observations \\(X_1,X_2,....,X_k\\) considering all possible paths (\\(S_1=s_1,S_2=s_2,\\ldots, S_k=i\\)), (i.e. all possible paths going from step 1 to \\(k\\), and ending with state \\(i\\)):\n\\[\n\\delta_i(k) = max_{s_1,s_2,....s_{k-1}}pr(X_1,X_2,\\ldots,X_k,S_1=s_1,S_2=s_2,\\ldots,S_k=i)/\\theta)\n\\]\nWe note \\(\\phi_i(k)\\) the argument of this maximum, i.e.\n\\[\n\\phi_i(k) = Argmax_{s_1,s_2,....s_{k-1}}pr(X_1,X_2,\\ldots,X_k,S_1=s_1,S_2=s_2,\\ldots,S_k=i)/\\theta)\n\\]\nHence, \\(\\delta_i(k)\\) corresponds to a maximum probability while \\(\\phi_i(k)\\) corresponds to the state \\(i\\) that maximizes it.\n\\(\\delta_j(k+1)\\) can be expressed as a function of \\(\\delta_i(k)\\):\n\\[\n\\delta_j(k+1) = [max_i\\delta_i(k)t_{ij}]*e_{jx_k+1}\n\\]\nStarting with\n\\[\n\\delta_1(i)=\\pi_ie_{ix_1}\n\\]\nwe can thus calculate recursively \\(\\delta_i(k)\\) and \\(\\phi_i(k)\\) for all possible states \\(i\\), as it is illustrated in figure \\(\\ref{Viterbi}\\).\n\n\n\nViterbi algorithm: an example of calculation and backward determination of the Viterbi path"
  },
  {
    "objectID": "hidden_states.html#applying-the-viterbi-aligorithm-in-practice",
    "href": "hidden_states.html#applying-the-viterbi-aligorithm-in-practice",
    "title": "3  Predicting hidden states",
    "section": "3.2 Applying the Viterbi aligorithm in practice",
    "text": "3.2 Applying the Viterbi aligorithm in practice\nNow we calculate the latent series according to the four different sets of initial parameters. We do not calculate them using the fitted HMMs because they are very close to each other and we want to show the effect of parameters on the inferred states sequence.\n\nlatent_series_LTP_UCS=as.factor(viterbi(hmm_init_LTP_UCS,obsWood))\nlatent_series_LTP_CS=as.factor(viterbi(hmm_init_LTP_CS,obsWood))\nlatent_series_HTP_UCS=as.factor(viterbi(hmm_init_HTP_UCS,obsWood))\nlatent_series_HTP_CS=as.factor(viterbi(hmm_init_HTP_CS,obsWood))\nlist_latent_series=list(latent_series_LTP_UCS,\n                        latent_series_LTP_CS,\n                        latent_series_HTP_UCS,\n                        latent_series_HTP_CS)\n\n\n\n\n\n\nVarious latent series of states (according to parameterization)\n\n\n\n\nUnsurprisingly, when transition probabilities are higher (HTP_* vs LTP_*), the segments tend to be shorter. The emission probabilities also have an effect on the segments’ lengths: contrasted states (in the case LTP_CS vs LTP_UCS) hence correspond to shorter segments."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HMM_tutorial",
    "section": "",
    "text": "Preface\nThis is a short book about Hidden Markov Models, their principles, and how they can be fitted and applied in practical cases.\nIt is organised in 4 chapters:\n\nThe introduction explains the principle of Hidden Markov Models through a simple probabilistic model involving boxes and balls.\nThe fitting part explains how a Hidden Markov Model is fit to an observed series of data to estimate its parameters.\nThe hidden states part explains how Hidden Markov Model are used to predict the hidden states behind a series of observations.\nThe case studies part shows how Hidden Markov Models can be used in two instances: one is the interpretation of geomorphic series along a river, the second is the interpretation of discursive content."
  },
  {
    "objectID": "continuous_HMMs.html",
    "href": "continuous_HMMs.html",
    "title": "5  Continuous Hidden Markov Models",
    "section": "",
    "text": "6 Introduction\n\nsource(\"scripts/scripts_Zucchini_and_McDonald_2009.R\")\n# number of wood rafts\ndata= readr::read_csv(\"data/wood_abundance.csv\")\npk=data$PK\nx=data$nbre_embacle\n\n\nplot(x, type=\"l\")\n\n\n\n\nAn example of observation series: occurrence of wood 500 meter-long sections of the river.\n\n\n\n\n\nlambda0=c(10,3,80)\ngamma0=matrix(rep(0.025,9),nrow=3)\ndiag(gamma0)=0.95\nparvect=pois.HMM.pn2pw(m=3,lambda=lambda0,gamma=gamma0)\nfitted.HMM=pois.HMM.mle(x,m=3,lambda0,gamma0)\n\nWarning in nlm(pois.HMM.mllk, parvect0, x = x, m = m): NA / Inf remplacé par la\nvaleur maximale positive\n\n\n\nprint(fitted.HMM)\n\n$lambda\n[1] 34.177617  2.585169 12.725369\n\n$gamma\n          [,1]      [,2]      [,3]\n[1,] 0.2216705 0.4205523 0.3577771\n[2,] 0.1159837 0.4795673 0.4044491\n[3,] 0.1957610 0.3936675 0.4105716\n\n$delta\n[1] 0.1652990 0.4355228 0.3991782\n\n$code\nNULL\n\n$mllk\n[1] 691.3476\n\n$AIC\n[1] 1400.695\n\n$BIC\n[1] 1429.823\n\n\n\nxsim=pois.HMM.generate_sample(n=length(x),m=3,lambda=fitted.HMM$lambda,fitted.HMM$gamma)\nplot(xsim, type=\"l\")\n\n\n\n\nAn example of simulated series: possible occurrence of wood rafts according to fitted model.\n\n\n\n\n\nfitted.HMM.EM=pois.HMM.EM(x,m=3,fitted.HMM$lambda,fitted.HMM$gamma, delta=NULL)\n\n\nstates=pois.HMM.viterbi(x,m=3,lambda=fitted.HMM.EM$lambda,fitted.HMM.EM$gamma,delta=NULL)\nprint(states)\n\n  [1] 2 2 2 1 2 2 2 3 2 3 3 3 2 2 1 3 2 3 3 3 1 3 2 3 2 3 2 2 3 2 3 3 3 2 3 3 2\n [38] 3 2 2 1 2 2 2 2 3 3 3 2 2 2 2 2 3 2 1 2 3 3 2 3 3 2 2 2 2 2 3 2 2 2 3 2 3\n [75] 2 2 2 3 2 2 2 2 2 3 2 3 2 1 2 3 1 1 3 3 1 3 3 3 1 3 2 1 3 3 1 1 2 2 3 3 3\n[112] 2 3 2 2 1 2 2 3 2 3 3 3 2 3 3 3 3 3 2 3 1 3 1 2 3 3 1 2 1 3 1 2 3 1 1 3 1\n[149] 2 3 1 3 3 1 1 1 1 1 3 1 2 2 2 2 3 2 3 1 2 3 3 2 3 3 3 3 1 2 2 2 1 2 2 2 2\n[186] 2 2 2\n\n\n\n#|fig-cap:\"Hidden states\"\nlayout(matrix(1:2, nrow=2))\nplot(pk,x,type=\"l\", main=\"number of wood rafts \")\nplot(pk,x, type=\"l\")\npoints(pk,fitted.HMM.EM$lambda[states], col=\"red\", type=\"l\", lty=2)\n\n\n\n\n\n\n7 The 2 states\n\nm=2\nlambda0=c(10,80)\ngamma0=matrix(rep(0.025,m^2),nrow=m)\ndiag(gamma0)=0.95\nparvect=pois.HMM.pn2pw(m=m,lambda=lambda0,gamma=gamma0)\nfitted.HMM=pois.HMM.mle(x,m=m,lambda0,gamma0)\n\nWarning in nlm(pois.HMM.mllk, parvect0, x = x, m = m): NA / Inf remplacé par la\nvaleur maximale positive\n\n\n\nprint(fitted.HMM)\n\n$lambda\n[1]  4.186209 23.368149\n\n$gamma\n          [,1]      [,2]\n[1,] 0.6841098 0.3158902\n[2,] 0.4752263 0.5247737\n\n$delta\n[1] 0.6007033 0.3992967\n\n$code\nNULL\n\n$mllk\n[1] 807.4495\n\n$AIC\n[1] 1622.899\n\n$BIC\n[1] 1635.845\n\n\n\nfitted.HMM.EM=pois.HMM.EM(x,m=m,fitted.HMM$lambda,fitted.HMM$gamma, delta=NULL)\n\n\nstates=pois.HMM.viterbi(x,m=m,lambda=fitted.HMM.EM$lambda,fitted.HMM.EM$gamma,delta=NULL)\nprint(states)\n\n  [1] 1 1 1 2 1 1 1 2 1 1 1 2 1 1 2 2 1 1 2 2 2 1 1 1 1 1 1 1 2 1 2 2 2 1 1 1 1\n [38] 1 1 1 2 1 1 1 1 1 2 2 1 1 1 1 1 2 1 2 1 2 2 1 2 2 1 1 1 1 1 2 1 1 1 1 1 1\n [75] 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 1 1 2 2 2\n[112] 1 2 1 1 2 1 1 1 1 1 2 2 1 2 1 1 1 1 1 1 2 2 2 1 2 2 2 1 2 1 2 1 2 2 2 1 2\n[149] 1 1 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 2 1 2 2 1 1 2 2 2 2 1 1 1 2 1 1 1 1\n[186] 1 1 1\n\n\n\nlayout(matrix(1:2, nrow=2))\nplot(pk,x,type=\"l\", main=\"number of wood rafts \")\nplot(pk,x, type=\"l\")\npoints(pk,fitted.HMM.EM$lambda[states], col=\"red\", type=\"l\", lty=2)"
  },
  {
    "objectID": "intro.html#definition",
    "href": "intro.html#definition",
    "title": "1  Introduction",
    "section": "1.1 Definition",
    "text": "1.1 Definition\nHidden Markov Models (HMMs) are models that apply to situations where a sequence of observable outputs (e.g. time or spatial series) are generated by hidden states.\nThey consist of two processes, a Markov transition process that describes the series of hidden states, and an emission process that describes the series of outputs produced by the system. This process is such that the probability of observing an output at a given point in the series depends only on the current hidden state."
  },
  {
    "objectID": "intro.html#a-generic-example-of-hmm-bags-and-balls",
    "href": "intro.html#a-generic-example-of-hmm-bags-and-balls",
    "title": "1  Introduction",
    "section": "1.2 A generic example of HMM: bags and balls",
    "text": "1.2 A generic example of HMM: bags and balls\n\n\n\nAn example of Hidden Markov Model with two states (bag A and bag B) and two outcomes (red ball -R- or white ball -W).\n\n\nLet’s consider the following example of HMM (which we will refer to as the bags and balls example throughout this tutorial).\nAn operator has two bags, A et B, filled with either red or white balls in different proportions. He carries out the following experiment:\n\nHe chooses one bag out of the two\nHe draws a ball out the bag he has just chosen, writes down its color on a sheet of paper, and then puts it back in the bag\nHe chooses to keep that same bag or to take the other one\nHe draws a ball out of the bag, writes down its color on the sheet of paper, and then puts it back in the bag\nHe chooses to keep that same bag or to take the other one,\netc.\n\nHe thus repeats the same process “drawing of ball, choosing of bag” \\(n\\) times. Then we are given the sheet of paper with the succession of colors written on it… We thus know the sequence of colors (the observations) but we do not know the sequence of bags (the hidden states). A possible series of hidden states and observations is shown in figure Figure 1.1.\nWe suppose that at any iteration \\(k\\), the operator chooses to keep the same bag or to change it only based on a constant rule which depends on the bag he had during iteration \\(k-1\\).\nThat is to say that the hidden state variable has the Markov property: its probability distribution at iteration \\(k\\) depends only on its value at iteration \\((k_1)\\), but is independent of all previous iterations (hence \\(pr(S_k/S_{k-1})\\) is independent of \\(S_1,S_2,...S_{k-2}\\)). It also means that \\(pr(S_k/S_{k-1})\\) is independent of \\(k\\).\n\n\n\nFigure 1.1: An example of hidden states and observations series"
  }
]